{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Indicators.csv' does not exist: b'Indicators.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f7ccb62e7c79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Indicators.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jinkyum\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jinkyum\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jinkyum\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jinkyum\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jinkyum\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Indicators.csv' does not exist: b'Indicators.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Indicators.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryName</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>IndicatorName</th>\n",
       "      <th>IndicatorCode</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arab World</td>\n",
       "      <td>ARB</td>\n",
       "      <td>Adolescent fertility rate (births per 1,000 wo...</td>\n",
       "      <td>SP.ADO.TFRT</td>\n",
       "      <td>1960</td>\n",
       "      <td>1.335609e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arab World</td>\n",
       "      <td>ARB</td>\n",
       "      <td>Age dependency ratio (% of working-age populat...</td>\n",
       "      <td>SP.POP.DPND</td>\n",
       "      <td>1960</td>\n",
       "      <td>8.779760e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arab World</td>\n",
       "      <td>ARB</td>\n",
       "      <td>Age dependency ratio, old (% of working-age po...</td>\n",
       "      <td>SP.POP.DPND.OL</td>\n",
       "      <td>1960</td>\n",
       "      <td>6.634579e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arab World</td>\n",
       "      <td>ARB</td>\n",
       "      <td>Age dependency ratio, young (% of working-age ...</td>\n",
       "      <td>SP.POP.DPND.YG</td>\n",
       "      <td>1960</td>\n",
       "      <td>8.102333e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arab World</td>\n",
       "      <td>ARB</td>\n",
       "      <td>Arms exports (SIPRI trend indicator values)</td>\n",
       "      <td>MS.MIL.XPRT.KD</td>\n",
       "      <td>1960</td>\n",
       "      <td>3.000000e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CountryName CountryCode                                      IndicatorName  \\\n",
       "0  Arab World         ARB  Adolescent fertility rate (births per 1,000 wo...   \n",
       "1  Arab World         ARB  Age dependency ratio (% of working-age populat...   \n",
       "2  Arab World         ARB  Age dependency ratio, old (% of working-age po...   \n",
       "3  Arab World         ARB  Age dependency ratio, young (% of working-age ...   \n",
       "4  Arab World         ARB        Arms exports (SIPRI trend indicator values)   \n",
       "\n",
       "    IndicatorCode  Year         Value  \n",
       "0     SP.ADO.TFRT  1960  1.335609e+02  \n",
       "1     SP.POP.DPND  1960  8.779760e+01  \n",
       "2  SP.POP.DPND.OL  1960  6.634579e+00  \n",
       "3  SP.POP.DPND.YG  1960  8.102333e+01  \n",
       "4  MS.MIL.XPRT.KD  1960  3.000000e+06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(data):\n",
    "    data=data[(data['Year'] >= 2000) & (data['Year'] < 2015) ]\n",
    "    for i in data.columns:\n",
    "        if data.isna().sum()[i]/len(data) >=0.8:\n",
    "            data=data.drop(columns=[i])\n",
    "\n",
    "    rel_columns= []\n",
    "    rel_columns_7=[]\n",
    "    unique_name = data['IndicatorName'].unique()\n",
    "    for i in range(len(unique_name)):\n",
    "        #print(unique_name[i])\n",
    "        indicator = data[data['IndicatorName'] == unique_name[i]]\n",
    "        indicator.rename(columns={\"Value\": unique_name[i]}, inplace = True)\n",
    "        indicator=indicator.drop(columns=['IndicatorCode', 'IndicatorName', 'CountryName'])\n",
    "        merged = pd.merge(target, indicator)\n",
    "        corr=merged.corr()\n",
    "        if corr['Value'][unique_name[i]] > 0.8 or corr['Value'][unique_name[i]] < -0.8:\n",
    "            rel_columns.append(unique_name[i])\n",
    "        if corr['Value'][unique_name[i]] > 0.7 or corr['Value'][unique_name[i]] < -0.7:\n",
    "            rel_columns_7.append(unique_name[i])\n",
    "    return rel_columns, rel_columns_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#일단 이렇게 되는데, 나중에 주석 풀고 하는게 나을듯\n",
    "#rel_columns_8,rel_columns_7=corr(data2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터 관련도 높은 feature 뽑는 것\n",
    "target_id = 'Adolescent fertility rate (births per 1,000 women ages 15-19)';\n",
    "\n",
    "target = data[data['IndicatorName']==target_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_columns_8 = ['Adolescent fertility rate (births per 1,000 women ages 15-19)',\n",
    " 'Birth rate, crude (per 1,000 people)',\n",
    " 'Net enrolment rate, secondary, both sexes (%)',\n",
    " 'Net enrolment rate, secondary, female (%)',\n",
    " 'Net enrolment rate, secondary, male (%)',\n",
    " 'Teenage mothers (% of women ages 15-19 who have had children or are currently pregnant)',\n",
    " 'Mortality rate, under-5, female (per 1,000 live births)',\n",
    " 'Mortality rate, under-5, male (per 1,000 live births)',\n",
    " 'Prevalence of anemia among children (% of children under 5)',\n",
    " 'Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total)',\n",
    " 'Cause of death, by non-communicable diseases (% of total)',\n",
    " 'Survey mean consumption or income per capita, bottom 40% of population (2005 PPP $ per day)',\n",
    " 'Survey mean consumption or income per capita, total population (2005 PPP $ per day)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_columns_7 = ['Adolescent fertility rate (births per 1,000 women ages 15-19)',\n",
    " 'Age dependency ratio (% of working-age population)',\n",
    " 'Age dependency ratio, young (% of working-age population)',\n",
    " 'Birth rate, crude (per 1,000 people)',\n",
    " 'Fertility rate, total (births per woman)',\n",
    " 'Life expectancy at birth, female (years)',\n",
    " 'Life expectancy at birth, male (years)',\n",
    " 'Life expectancy at birth, total (years)',\n",
    " 'Mortality rate, adult, female (per 1,000 female adults)',\n",
    " 'Mortality rate, infant (per 1,000 live births)',\n",
    " 'Mortality rate, under-5 (per 1,000)',\n",
    " 'Population, ages 0-14 (% of total)',\n",
    " 'Population, ages 15-64 (% of total)',\n",
    " 'Survival to age 65, female (% of cohort)',\n",
    " 'Survival to age 65, male (% of cohort)',\n",
    " 'Adjusted net enrolment rate, primary, both sexes (%)',\n",
    " 'Adjusted net enrolment rate, primary, female (%)',\n",
    " 'Gross enrolment ratio, secondary, both sexes (%)',\n",
    " 'Gross enrolment ratio, secondary, female (%)',\n",
    " 'Gross enrolment ratio, secondary, male (%)',\n",
    " 'Lower secondary completion rate, both sexes (%)',\n",
    " 'Net enrolment rate, secondary, both sexes (%)',\n",
    " 'Net enrolment rate, secondary, female (%)',\n",
    " 'Net enrolment rate, secondary, male (%)',\n",
    " 'Primary completion rate, both sexes (%)',\n",
    " 'Primary completion rate, female (%)',\n",
    " 'Survival rate to the last grade of primary education, both sexes (%)',\n",
    " 'Lower secondary completion rate, female (%)',\n",
    " 'Lower secondary completion rate, male (%)',\n",
    " 'Survival rate to the last grade of primary education, female (%)',\n",
    " 'Survival rate to the last grade of primary education, male (%)',\n",
    " 'Survival rate to Grade 5 of primary education, female (%)',\n",
    " 'Youth literacy rate, population 15-24 years, both sexes (%)',\n",
    " 'Youth literacy rate, population 15-24 years, male (%)',\n",
    " 'Teenage mothers (% of women ages 15-19 who have had children or are currently pregnant)',\n",
    " 'Wanted fertility rate (births per woman)',\n",
    " 'Access to electricity (% of population)',\n",
    " 'Access to electricity, rural (% of rural population)',\n",
    " 'Improved sanitation facilities (% of population with access)',\n",
    " 'Improved sanitation facilities, rural (% of rural population with access)',\n",
    " 'Improved sanitation facilities, urban (% of urban population with access)',\n",
    " 'Improved water source (% of population with access)',\n",
    " 'Improved water source, rural (% of rural population with access)',\n",
    " 'Lifetime risk of maternal death (%)',\n",
    " 'Maternal mortality ratio (modeled estimate, per 100,000 live births)',\n",
    " 'Mortality rate, infant, female (per 1,000 live births)',\n",
    " 'Mortality rate, infant, male (per 1,000 live births)',\n",
    " 'Mortality rate, neonatal (per 1,000 live births)',\n",
    " 'Mortality rate, under-5, female (per 1,000 live births)',\n",
    " 'Mortality rate, under-5, male (per 1,000 live births)',\n",
    " 'Prevalence of anemia among children (% of children under 5)',\n",
    " 'Prevalence of anemia among pregnant women (%)',\n",
    " 'Renewable energy consumption (% of total final energy consumption)',\n",
    " 'Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total)',\n",
    " 'Cause of death, by non-communicable diseases (% of total)',\n",
    " 'Survey mean consumption or income per capita, bottom 40% of population (2005 PPP $ per day)',\n",
    " 'Survey mean consumption or income per capita, total population (2005 PPP $ per day)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당하는 indicator해서 그 데이터 프레임 사용하기 편하게 바꿔주는거 \n",
    "def transformDf(data, ind):\n",
    "    data2000=data[(data['Year'] >= 2000) & (data['Year'] < 2015) ]\n",
    "    col8 = data2000[(data2000['IndicatorName'].isin(ind))]\n",
    "    col8=col8.reset_index(drop=True)\n",
    "    name_list = list()\n",
    "\n",
    "    for i in col8.index:\n",
    "        name_list.append( col8.loc[i,'CountryName'] + \"_\" + str(col8.loc[i,'Year']))\n",
    "    col8[\"name\"] = name_list\n",
    "    \n",
    "    col8=col8.pivot_table(values=\"Value\",index=\"name\",columns=\"IndicatorName\")\n",
    "\n",
    "    col8=col8.dropna(subset=[target_id])\n",
    "    return col8\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_grade(merged, k):\n",
    "    min_value = min(merged[target_id])\n",
    "    max_value = max(merged[target_id])\n",
    "    value=(max_value-min_value)/k\n",
    "    for i in range(k):\n",
    "        for j in merged.index:\n",
    "            if merged.loc[j, target_id] >=min_value + value*i and merged.loc[j, target_id] <= min_value + value*(i+1):\n",
    "                merged.loc[j, 'grade'] = i\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 missing value추가하는거 넣어야될듯\n",
    "1. 아예 드랍\n",
    "2. missing value을 채우고 난 뒤, 그래도 못 채우면 드랍\n",
    "    예시)- 한개 column 이 na 일 때 가정, indicator=[a,b,c]\n",
    "    \n",
    "        1) if a[한국] is null\n",
    "            b[한국] & c[한국] 비슷한 다른 나라 참조 해서 a[한국]=a[다른나라]\n",
    "            이렇게 알고리즘 짜고 그래도 못할시에 na로 일단 남겨 놓고, 마지막에 검사해서 드랍\n",
    "            \n",
    "        2) 비슷한 연도이니까, 거의 안그럴 거 같은데 2001년은 null이고 2000은 null 이 아니면 2000년도꺼 그냥 쓴다던지,,, \n",
    "        그 regression? 그런거 써서 processing 에서도 그 predict 써서 채워넣돈지\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingMissing(df):\n",
    "    \n",
    "    #전체 개수랑 null 값이 비슷 하게 나오는 경우에는 그냥 아예 그 컬럼을 안쓰는게 나을 거 가음\n",
    "    for i in df.columns:\n",
    "        if df.isna().sum()[i]/len(df) >=0.7:\n",
    "\n",
    "            df=df.drop(columns=[i])\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleansing dataset\n",
    "def robust(X):\n",
    "    # cleansing missing data\n",
    "    X.dropna(axis=0, thresh=1, inplace=True)\n",
    "    # cleansing outliers\n",
    "    robust = preprocessing.RobustScaler().fit(X)\n",
    "\n",
    "    X_robust = robust.transform(X)\n",
    "    X_robust = pd.DataFrame(X, columns=X.columns, index=list(X.index.values))\n",
    "    return X_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data (MinMaxScaler)\n",
    "def minmax(X):\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    fit = scaler.fit(X)\n",
    "\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=list(X.index.values))\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data (MinMaxScaler)\n",
    "def standard(X):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    fit = scaler.fit(X)\n",
    "\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=list(X.index.values))\n",
    "    return X_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillnaX(X):\n",
    "    for i in X.columns:\n",
    "        df[i] = df[i].fillna(df[i].mean())\n",
    "    return X                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#나중에 dbscan이나 EM 도 한파일에 있어야되서 함수 명 바꿧어\n",
    "def runKMeans(df, k,max_iter, n_init):\n",
    "    \n",
    "    kmeans= KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=max_iter, \n",
    "               n_clusters=k, n_init=n_init, n_jobs=1, precompute_distances='auto', \n",
    "               random_state=None, tol=0.0001, verbose=0)\n",
    "    df=make_grade(df,k)\n",
    "    df=preprocessingMissing(df)\n",
    "    \n",
    "    X= df.drop(['grade',target_id], 1)\n",
    "    #일단은 다 dropna 처리\n",
    "    X=X.dropna()\n",
    "    y=df['grade']\n",
    "\n",
    "    kmeans.fit(X)\n",
    "    predict=kmeans.predict(X)\n",
    "    return calcAccuracy(predict, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calcAccuracy(y_pred, x,y):\n",
    "    '''\n",
    "    print accuracy of prediction\n",
    "    '''\n",
    "    correct=0\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        #predict_me= np.array(x[i].astype(float))\n",
    "        #predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "        #prediction = kmeans.predict(predict_me)\n",
    "        #여기서는 predict 값을 넘겨서 [i]로 해야돼\n",
    "        if y_pred[i] == y[i]:\n",
    "            correct += 1\n",
    "    print(correct/len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDBScan(df,eps, min_samples, metric):\n",
    "    \n",
    "    \n",
    "    X=df.drop(columns=['grade',target_id])\n",
    "    y=df['grade']\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    X=df.drop(columns=[target_id])\n",
    "    y=df[target_id]\n",
    "    \"\"\"\n",
    "    \n",
    "    X = robust(X)\n",
    "    X= fillnaX(X)\n",
    "\n",
    "    db = DBSCAN(eps = eps, min_samples = min_samples, metric= metric).fit(X) \n",
    "\n",
    "    cluster_labels = db.labels_\n",
    "    n_clusters = len(set(cluster_labels))\n",
    "    \n",
    "    return calcAccuracy(cluster_labels, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=transformDf(data,rel_columns_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6411889596602972\n"
     ]
    }
   ],
   "source": [
    "runKMeans(df,2,800,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=[10, 20, 30, 40, 50 ]\n",
    "min_samples=[ 3, 5, 10, 15, 20, 30, 50, 100 ]\n",
    "dis_measure = ['euclidean', 'hamming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-75020b7fd80c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrunDBScan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-7f3785c928d9>\u001b[0m in \u001b[0;36mrunDBScan\u001b[1;34m(df, eps, min_samples, metric)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfillnaX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mcluster_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parksoyoung\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\cluster\\dbscan_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m         clust = dbscan(X, sample_weight=sample_weight,\n\u001b[0;32m    351\u001b[0m                        **self.get_params())\n",
      "\u001b[1;32mc:\\users\\parksoyoung\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parksoyoung\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# 바꿔야함\n",
    "df = df.fillna(0)\n",
    "runDBScan(df,10,3,'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "선택지\n",
    "1. rel_columns_8 or 7\n",
    "2. 몇개 미만이면 제외할지\n",
    "3. kmeans parameter\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
